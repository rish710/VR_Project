{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df336a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.corpus import wordnet\n",
    "from bert_score import score as bert_score\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b23043",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine similarity model\n",
    "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588f77f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exact match\n",
    "def exact_match(y_true, y_pred):\n",
    "    return [int(a.strip().lower() == b.strip().lower()) for a, b in zip(y_true, y_pred)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e3f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE Score\n",
    "def rouge_l_score(y_true, y_pred):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, pred)['rougeL'].fmeasure for ref, pred in zip(y_true, y_pred)]\n",
    "    return sum(scores) / len(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122aacd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity score\n",
    "def cosine_similarity_score(y_true, y_pred):\n",
    "    embeddings1 = sbert_model.encode(y_true, convert_to_tensor=True)\n",
    "    embeddings2 = sbert_model.encode(y_pred, convert_to_tensor=True)\n",
    "    cos_sim = util.cos_sim(embeddings1, embeddings2)\n",
    "    return cos_sim.diag().mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f3336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_file(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    y_true = df['true_answer'].astype(str).tolist()\n",
    "    y_pred = df['predicted_answer'].astype(str).tolist()\n",
    "\n",
    "    em = exact_match(y_true, y_pred)\n",
    "    acc = accuracy_score(em, [1]*len(em))\n",
    "    precision = precision_score(em, [1]*len(em), zero_division=0)\n",
    "    recall = recall_score(em, [1]*len(em), zero_division=0)\n",
    "    f1 = f1_score(em, [1]*len(em), zero_division=0)\n",
    "\n",
    "    rouge = rouge_l_score(y_true, y_pred)\n",
    "\n",
    "    syn_acc = accuracy_score(synonym_match(y_true, y_pred), [1]*len(y_true))\n",
    "\n",
    "    # BERTScore\n",
    "    bert_p, bert_r, bert_f1 = bert_score(y_pred, y_true, lang='en', rescale_with_baseline=True)\n",
    "\n",
    "    cos_sim = cosine_similarity_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"Exact Match Accuracy\":round(acc * 100, 2),\n",
    "        # Proportion of predictions that exactly match the true answers.\n",
    "        \n",
    "        \"Synonym Accuracy\" :round(syn_acc * 100, 2),\n",
    "        # Proportion of predicted answers that are synonyms of the true answers.\n",
    "\n",
    "        \"Exact Match Precision\":round(precision * 100, 2),\n",
    "        # Proportion of exact matches among all predicted matches.\n",
    "        \n",
    "        \"Exact Match Recall\":round(recall * 100, 2),\n",
    "        # Proportion of exact matches among all true answers.\n",
    "\n",
    "        \"Exact Match F1\":round(f1 * 100, 2),\n",
    "        # Harmonic mean of exact match precision and recall.\n",
    "\n",
    "        \"ROUGE-L F1\" :round(rouge * 100, 2),\n",
    "        # Measures overlap based on the longest common subsequence between predicted and true answers.\n",
    "\n",
    "        \"BERTScore Precision\" :round(bert_p.mean().item() * 100, 2),\n",
    "        # Measures how much of the predicted answer’s meaning matches the true answer using contextual embeddings.\n",
    "\n",
    "        \"BERTScore Recall\" :round(bert_r.mean().item() * 100, 2),\n",
    "        # Measures how much of the true answer’s meaning is captured by the prediction using contextual embeddings.\n",
    "\n",
    "        \"BERTScore F1\"  :round(bert_f1.mean().item() * 100, 2),\n",
    "        # Harmonic mean of BERTScore precision and recall, indicating overall semantic similarity.\n",
    "\n",
    "        \"Cosine Similarity\" : round(cos_sim * 100, 2),\n",
    "        # Cosine of the angle between the embedding vectors of predicted and true answers, representing semantic closeness.\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cab90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction CSVs of Models\n",
    "\n",
    "csv_files = [\n",
    "    \"Evaluation/predictions.csv\",\n",
    "    \"Evaluation/predictions.csv\",\n",
    "    \"Evaluation/predictions.csv\",\n",
    "    \"Evaluation/predictions.csv\",\n",
    "    \"Evaluation/predictions.csv\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Evaluate and print results\n",
    "for csv_file in csv_files:\n",
    "    print(f\"\\n{csv_file}\")\n",
    "    scores = evaluate_file(csv_file)\n",
    "    for metric, value in scores.items():\n",
    "        print(f\"{metric:<25}: {value}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
