{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd087033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86033502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted listings_0.json.gz → listings_0.json\n",
      "Extracted listings_1.json.gz → listings_1.json\n",
      "Extracted listings_2.json.gz → listings_2.json\n",
      "Extracted listings_3.json.gz → listings_3.json\n",
      "Extracted listings_4.json.gz → listings_4.json\n",
      "Extracted listings_5.json.gz → listings_5.json\n",
      "Extracted listings_6.json.gz → listings_6.json\n",
      "Extracted listings_7.json.gz → listings_7.json\n",
      "Extracted listings_8.json.gz → listings_8.json\n",
      "Extracted listings_9.json.gz → listings_9.json\n",
      "Extracted listings_a.json.gz → listings_a.json\n",
      "Extracted listings_b.json.gz → listings_b.json\n",
      "Extracted listings_c.json.gz → listings_c.json\n",
      "Extracted listings_d.json.gz → listings_d.json\n",
      "Extracted listings_e.json.gz → listings_e.json\n",
      "Extracted listings_f.json.gz → listings_f.json\n",
      "All extractions complete.\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'abo-listings/listings/metadata/'\n",
    "output_dir = 'abo-listings/listings/extracted_metadata/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json.gz'):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_filename = filename[:-3]  # remove the .gz extension\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        \n",
    "        with gzip.open(input_path, 'rb') as f_in:\n",
    "            with open(output_path, 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)\n",
    "        \n",
    "        print(f\"Extracted {filename} → {output_filename}\")\n",
    "\n",
    "print(\"All extractions complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d95fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listings_0.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_1.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_2.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_3.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_4.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_5.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_6.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_7.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_8.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_9.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_a.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_b.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_c.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_d.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_e.json loaded as 9232 separate JSON objects (line by line).\n",
      "listings_f.json loaded as 9222 separate JSON objects (line by line).\n",
      "JSON structure check complete.\n"
     ]
    }
   ],
   "source": [
    "json_dir = 'abo-listings/listings/extracted_metadata/'\n",
    "\n",
    "for filename in os.listdir(json_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        file_path = os.path.join(json_dir, filename)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read().strip()\n",
    "                \n",
    "                if content.startswith('['):\n",
    "                    data = json.loads(content)\n",
    "                    print(f\"{filename} loaded as JSON array. Length: {len(data)}\")\n",
    "                else:\n",
    "                    items = []\n",
    "                    for line in content.splitlines():\n",
    "                        line = line.strip()\n",
    "                        if line:\n",
    "                            items.append(json.loads(line))\n",
    "                    print(f\"{filename} loaded as {len(items)} separate JSON objects (line by line).\")\n",
    "                    \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"{filename} failed to load: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"{filename} encountered an error: {e}\")\n",
    "\n",
    "print(\"JSON structure check complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af394b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting abo-listings/listings/extracted_metadata/listings_3.json\n",
      "\n",
      "--- First 500 characters ---\n",
      "{\"brand\": [{\"language_tag\": \"en_IN\", \"value\": \"Amazon Brand - Solimo\"}], \"bullet_point\": [{\"language_tag\": \"en_IN\", \"value\": \"Snug fit for Samsung Galaxy A10s, with perfect cut-outs for volume buttons, audio and charging ports\"}, {\"language_tag\": \"en_IN\", \"value\": \"Compatible with Samsung Galaxy A10s\"}, {\"language_tag\": \"en_IN\", \"value\": \"Easy to put & take off with perfect cutouts for volume buttons, audio & charging ports.\"}, {\"language_tag\": \"en_IN\", \"value\": \"Stylish design and appearance, e\n",
      "\n",
      "---------------------------\n",
      "Failed to load as single JSON: Extra data: line 2 column 1 (char 2490)\n",
      "\n",
      "Trying to parse line by line...\n",
      "Parsed 9232 JSON objects (line by line)\n",
      "First item type: <class 'dict'>\n",
      "First item keys: ['brand', 'bullet_point', 'color', 'item_id', 'item_name', 'item_weight', 'material', 'model_name', 'model_number', 'product_type', 'main_image_id', 'other_image_id', 'item_keywords', 'country', 'marketplace', 'domain_name', 'node']\n"
     ]
    }
   ],
   "source": [
    "file_path = 'abo-listings/listings/extracted_metadata/listings_3.json'\n",
    "\n",
    "print(f\"Inspecting {file_path}\")\n",
    "\n",
    "# Read the raw content\n",
    "with open(file_path, 'r', encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "\n",
    "# Show first 500 characters for manual inspection\n",
    "print(\"\\n--- First 500 characters ---\")\n",
    "print(raw[:500])\n",
    "print(\"\\n---------------------------\")\n",
    "\n",
    "# Try loading as a single JSON object\n",
    "try:\n",
    "    data = json.loads(raw)\n",
    "    print(\" Loaded as single JSON object\")\n",
    "    print(f\"Type: {type(data)}\")\n",
    "    if isinstance(data, dict):\n",
    "        print(f\"Top-level keys: {list(data.keys())}\")\n",
    "    elif isinstance(data, list):\n",
    "        print(f\"List length: {len(data)}\")\n",
    "        print(f\"First item type: {type(data[0])}\")\n",
    "        if isinstance(data[0], dict):\n",
    "            print(f\"First item keys: {list(data[0].keys())}\")\n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"Failed to load as single JSON: {e}\")\n",
    "\n",
    "    # Try line by line\n",
    "    print(\"\\nTrying to parse line by line...\")\n",
    "    items = []\n",
    "    for line in raw.splitlines():\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "                items.append(obj)\n",
    "            except Exception as sub_e:\n",
    "                print(f\"Failed to parse line: {sub_e}\")\n",
    "                break\n",
    "\n",
    "    print(f\"Parsed {len(items)} JSON objects (line by line)\")\n",
    "    if items:\n",
    "        print(f\"First item type: {type(items[0])}\")\n",
    "        if isinstance(items[0], dict):\n",
    "            print(f\"First item keys: {list(items[0].keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0748c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing abo-listings/listings/extracted_metadata\\listings_0.json → abo-listings/listings/filtered_metadata\\listings_0.csv\n",
      " → Total matching records: 4258\n",
      " → Saved 4258 records to abo-listings/listings/filtered_metadata\\listings_0.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_1.json → abo-listings/listings/filtered_metadata\\listings_1.csv\n",
      " → Total matching records: 4208\n",
      " → Saved 4208 records to abo-listings/listings/filtered_metadata\\listings_1.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_2.json → abo-listings/listings/filtered_metadata\\listings_2.csv\n",
      " → Total matching records: 4223\n",
      " → Saved 4223 records to abo-listings/listings/filtered_metadata\\listings_2.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_3.json → abo-listings/listings/filtered_metadata\\listings_3.csv\n",
      " → Total matching records: 4193\n",
      " → Saved 4193 records to abo-listings/listings/filtered_metadata\\listings_3.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_4.json → abo-listings/listings/filtered_metadata\\listings_4.csv\n",
      " → Total matching records: 4134\n",
      " → Saved 4134 records to abo-listings/listings/filtered_metadata\\listings_4.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_5.json → abo-listings/listings/filtered_metadata\\listings_5.csv\n",
      " → Total matching records: 4271\n",
      " → Saved 4271 records to abo-listings/listings/filtered_metadata\\listings_5.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_6.json → abo-listings/listings/filtered_metadata\\listings_6.csv\n",
      " → Total matching records: 4209\n",
      " → Saved 4209 records to abo-listings/listings/filtered_metadata\\listings_6.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_7.json → abo-listings/listings/filtered_metadata\\listings_7.csv\n",
      " → Total matching records: 4166\n",
      " → Saved 4166 records to abo-listings/listings/filtered_metadata\\listings_7.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_8.json → abo-listings/listings/filtered_metadata\\listings_8.csv\n",
      " → Total matching records: 4207\n",
      " → Saved 4207 records to abo-listings/listings/filtered_metadata\\listings_8.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_9.json → abo-listings/listings/filtered_metadata\\listings_9.csv\n",
      " → Total matching records: 4152\n",
      " → Saved 4152 records to abo-listings/listings/filtered_metadata\\listings_9.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_a.json → abo-listings/listings/filtered_metadata\\listings_a.csv\n",
      " → Total matching records: 4232\n",
      " → Saved 4232 records to abo-listings/listings/filtered_metadata\\listings_a.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_b.json → abo-listings/listings/filtered_metadata\\listings_b.csv\n",
      " → Total matching records: 4279\n",
      " → Saved 4279 records to abo-listings/listings/filtered_metadata\\listings_b.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_c.json → abo-listings/listings/filtered_metadata\\listings_c.csv\n",
      " → Total matching records: 4274\n",
      " → Saved 4274 records to abo-listings/listings/filtered_metadata\\listings_c.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_d.json → abo-listings/listings/filtered_metadata\\listings_d.csv\n",
      " → Total matching records: 4178\n",
      " → Saved 4178 records to abo-listings/listings/filtered_metadata\\listings_d.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_e.json → abo-listings/listings/filtered_metadata\\listings_e.csv\n",
      " → Total matching records: 4127\n",
      " → Saved 4127 records to abo-listings/listings/filtered_metadata\\listings_e.csv\n",
      "\n",
      "Processing abo-listings/listings/extracted_metadata\\listings_f.json → abo-listings/listings/filtered_metadata\\listings_f.csv\n",
      " → Total matching records: 4255\n",
      " → Saved 4255 records to abo-listings/listings/filtered_metadata\\listings_f.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_dir = 'abo-listings/listings/extracted_metadata'\n",
    "output_dir = 'abo-listings/listings/filtered_metadata'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "header = [\n",
    "    'main_image_id', 'overall_description', 'colour_description', 'other_description', 'material_description'\n",
    "]\n",
    "\n",
    "# Function to filter 'value' fields by language_tag (or accept if no language_tag)\n",
    "def get_filtered_values(entries):\n",
    "    filtered_values = []\n",
    "    for entry in entries:\n",
    "        value = entry.get('value')\n",
    "        language_tag = entry.get('language_tag')\n",
    "        if value and (language_tag is None or language_tag in ['en_IN', 'en_US']):\n",
    "            filtered_values.append(value)\n",
    "    return filtered_values\n",
    "\n",
    "# Function to filter 'standardized_values' by language_tag (or accept if no language_tag)\n",
    "def get_filtered_standardized_values(color_entries):\n",
    "    filtered_values = []\n",
    "    for entry in color_entries:\n",
    "        language_tag = entry.get('language_tag')\n",
    "        if language_tag is None or language_tag in ['en_IN', 'en_US']:\n",
    "            std_values = entry.get('standardized_values', [])\n",
    "            filtered_values.extend(std_values)\n",
    "    return filtered_values\n",
    "\n",
    "# Process each JSON file in the input directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith('.json'):\n",
    "        input_file = os.path.join(input_dir, filename)\n",
    "        output_file = os.path.join(output_dir, filename.replace('.json', '.csv'))\n",
    "\n",
    "        print(f\"Processing {input_file} → {output_file}\")\n",
    "\n",
    "        # Load line-delimited JSON\n",
    "        records = []\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    records.append(json.loads(line))\n",
    "                except json.JSONDecodeError as e:\n",
    "                    print(f\"Failed to load a line in {filename}: {e}\")\n",
    "\n",
    "        required_keys = ['brand', 'bullet_point', 'color', 'model_name', 'item_name', \n",
    "                         'product_type', 'main_image_id', 'item_keywords', 'country']\n",
    "\n",
    "        filtered_records = [\n",
    "            record for record in records\n",
    "            if all(key in record for key in required_keys)\n",
    "            and record.get('country') in ['IN', 'US']\n",
    "            # and 'item_dimensions' not in record\n",
    "        ]\n",
    "\n",
    "        print(f\" → Total matching records: {len(filtered_records)}\")\n",
    "\n",
    "        with open(output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "            writer = csv.writer(csvfile)\n",
    "            writer.writerow(header) \n",
    "\n",
    "            for record in filtered_records:\n",
    "                overall_description = get_filtered_values(record.get('bullet_point', []))\n",
    "                colour_description = []\n",
    "                colour_description.extend(get_filtered_standardized_values(record.get('color', [])))\n",
    "                colour_description.extend(get_filtered_values(record.get('color', [])))\n",
    "                other_description = []\n",
    "                for field in ['product_type', 'item_keywords']:\n",
    "                    other_description.extend(get_filtered_values(record.get(field, [])))\n",
    "                material_description = []\n",
    "                if 'material' in record:\n",
    "                    material_description.extend(get_filtered_values(record.get('material', [])))\n",
    "\n",
    "                row = [\n",
    "                    record.get('main_image_id'),\n",
    "                    '; '.join(overall_description),\n",
    "                    '; '.join(colour_description),\n",
    "                    '; '.join(other_description),\n",
    "                    '; '.join(material_description)\n",
    "                ]\n",
    "\n",
    "                writer.writerow(row)\n",
    "\n",
    "        print(f\" → Saved {len(filtered_records)} records to {output_file}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fcd20b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\rog\\anaconda3\\envs\\mlproject\\lib\\site-packages (24.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "   ---------------------------------------- 0.0/1.8 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.0/1.8 MB 7.3 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 1.6/1.8 MB 4.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.8/1.8 MB 3.8 MB/s eta 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\rog\\anaconda3\\envs\\MLProject\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\n",
      "  Downloading google_genai-1.13.0-py3-none-any.whl.metadata (32 kB)\n",
      "Collecting anyio<5.0.0,>=4.8.0 (from google-genai)\n",
      "  Downloading anyio-4.9.0-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth<3.0.0,>=2.14.1 (from google-genai)\n",
      "  Downloading google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting httpx<1.0.0,>=0.28.1 (from google-genai)\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting pydantic<3.0.0,>=2.0.0 (from google-genai)\n",
      "  Downloading pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Collecting requests<3.0.0,>=2.28.1 (from google-genai)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting websockets<15.1.0,>=13.0.0 (from google-genai)\n",
      "  Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in c:\\users\\rog\\anaconda3\\envs\\mlproject\\lib\\site-packages (from google-genai) (4.12.2)\n",
      "Collecting idna>=2.8 (from anyio<5.0.0,>=4.8.0->google-genai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sniffio>=1.1 (from anyio<5.0.0,>=4.8.0->google-genai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting certifi (from httpx<1.0.0,>=0.28.1->google-genai)\n",
      "  Downloading certifi-2025.4.26-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting httpcore==1.* (from httpx<1.0.0,>=0.28.1->google-genai)\n",
      "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai)\n",
      "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.0.0->google-genai)\n",
      "  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.28.1->google-genai)\n",
      "  Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl.metadata (36 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.28.1->google-genai)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-genai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Downloading google_genai-1.13.0-py3-none-any.whl (164 kB)\n",
      "Downloading anyio-4.9.0-py3-none-any.whl (100 kB)\n",
      "Downloading google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Downloading pydantic_core-2.33.2-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ----- ---------------------------------- 0.3/2.0 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------- ----------------------------- 0.5/2.0 MB 123.8 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 0.8/2.0 MB 103.5 kB/s eta 0:00:12\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 1.0/2.0 MB 92.0 kB/s eta 0:00:10\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------- ------------- 1.3/2.0 MB 94.5 kB/s eta 0:00:07\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   -------------------------------- ------- 1.6/2.0 MB 97.5 kB/s eta 0:00:04\n",
      "   ------------------------------------- -- 1.8/2.0 MB 96.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.8/2.0 MB 96.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.8/2.0 MB 96.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.8/2.0 MB 96.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 1.8/2.0 MB 96.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------- 2.0/2.0 MB 98.9 kB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading websockets-15.0.1-cp313-cp313-win_amd64.whl (176 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Downloading certifi-2025.4.26-py3-none-any.whl (159 kB)\n",
      "Downloading charset_normalizer-3.4.2-cp313-cp313-win_amd64.whl (105 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Downloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: websockets, urllib3, typing-inspection, sniffio, pydantic-core, pyasn1, idna, h11, charset-normalizer, certifi, cachetools, annotated-types, rsa, requests, pydantic, pyasn1-modules, httpcore, anyio, httpx, google-auth, google-genai\n",
      "Successfully installed annotated-types-0.7.0 anyio-4.9.0 cachetools-5.5.2 certifi-2025.4.26 charset-normalizer-3.4.2 google-auth-2.40.1 google-genai-1.13.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 requests-2.32.3 rsa-4.9.1 sniffio-1.3.1 typing-inspection-0.4.0 urllib3-2.4.0 websockets-15.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83fa80a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Client', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', '__version__', '_api_client', '_api_module', '_base_url', '_common', '_extra_utils', '_live_converters', '_replay_api_client', '_transformers', 'batches', 'caches', 'chats', 'client', 'errors', 'files', 'live', 'models', 'operations', 'pagers', 'tunings', 'types', 'version']\n",
      "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__firstlineno__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__static_attributes__', '__str__', '__subclasshook__', '__weakref__', '_get_api_client', 'aio', 'batches', 'caches', 'chats', 'files', 'models', 'operations', 'tunings', 'vertexai']\n",
      "['ActivityEnd', 'ActivityEndDict', 'ActivityEndOrDict', 'ActivityHandling', 'ActivityStart', 'ActivityStartDict', 'ActivityStartOrDict', 'AdapterSize', 'Any', 'AudioTranscriptionConfig', 'AudioTranscriptionConfigDict', 'AudioTranscriptionConfigOrDict', 'AutomaticActivityDetection', 'AutomaticActivityDetectionDict', 'AutomaticActivityDetectionOrDict', 'AutomaticFunctionCallingConfig', 'AutomaticFunctionCallingConfigDict', 'AutomaticFunctionCallingConfigOrDict', 'BatchJob', 'BatchJobDestination', 'BatchJobDestinationDict', 'BatchJobDestinationOrDict', 'BatchJobDict', 'BatchJobOrDict', 'BatchJobSource', 'BatchJobSourceDict', 'BatchJobSourceOrDict', 'Blob', 'BlobDict', 'BlobImageUnion', 'BlobImageUnionDict', 'BlobOrDict', 'BlockedReason', 'CachedContent', 'CachedContentDict', 'CachedContentOrDict', 'CachedContentUsageMetadata', 'CachedContentUsageMetadataDict', 'CachedContentUsageMetadataOrDict', 'Callable', 'CancelBatchJobConfig', 'CancelBatchJobConfigDict', 'CancelBatchJobConfigOrDict', 'Candidate', 'CandidateDict', 'CandidateOrDict', 'Citation', 'CitationDict', 'CitationMetadata', 'CitationMetadataDict', 'CitationMetadataOrDict', 'CitationOrDict', 'CodeExecutionResult', 'CodeExecutionResultDict', 'CodeExecutionResultOrDict', 'ComputeTokensConfig', 'ComputeTokensConfigDict', 'ComputeTokensConfigOrDict', 'ComputeTokensResponse', 'ComputeTokensResponseDict', 'ComputeTokensResponseOrDict', 'Content', 'ContentDict', 'ContentEmbedding', 'ContentEmbeddingDict', 'ContentEmbeddingOrDict', 'ContentEmbeddingStatistics', 'ContentEmbeddingStatisticsDict', 'ContentEmbeddingStatisticsOrDict', 'ContentListUnion', 'ContentListUnionDict', 'ContentOrDict', 'ContentUnion', 'ContentUnionDict', 'ContextWindowCompressionConfig', 'ContextWindowCompressionConfigDict', 'ContextWindowCompressionConfigOrDict', 'ControlReferenceConfig', 'ControlReferenceConfigDict', 'ControlReferenceConfigOrDict', 'ControlReferenceImage', 'ControlReferenceImageDict', 'ControlReferenceImageOrDict', 'ControlReferenceType', 'CountTokensConfig', 'CountTokensConfigDict', 'CountTokensConfigOrDict', 'CountTokensResponse', 'CountTokensResponseDict', 'CountTokensResponseOrDict', 'CreateBatchJobConfig', 'CreateBatchJobConfigDict', 'CreateBatchJobConfigOrDict', 'CreateCachedContentConfig', 'CreateCachedContentConfigDict', 'CreateCachedContentConfigOrDict', 'CreateFileConfig', 'CreateFileConfigDict', 'CreateFileConfigOrDict', 'CreateFileResponse', 'CreateFileResponseDict', 'CreateFileResponseOrDict', 'CreateTuningJobConfig', 'CreateTuningJobConfigDict', 'CreateTuningJobConfigOrDict', 'DatasetDistribution', 'DatasetDistributionDict', 'DatasetDistributionDistributionBucket', 'DatasetDistributionDistributionBucketDict', 'DatasetDistributionDistributionBucketOrDict', 'DatasetDistributionOrDict', 'DatasetStats', 'DatasetStatsDict', 'DatasetStatsOrDict', 'DeleteBatchJobConfig', 'DeleteBatchJobConfigDict', 'DeleteBatchJobConfigOrDict', 'DeleteCachedContentConfig', 'DeleteCachedContentConfigDict', 'DeleteCachedContentConfigOrDict', 'DeleteCachedContentResponse', 'DeleteCachedContentResponseDict', 'DeleteCachedContentResponseOrDict', 'DeleteFileConfig', 'DeleteFileConfigDict', 'DeleteFileConfigOrDict', 'DeleteFileResponse', 'DeleteFileResponseDict', 'DeleteFileResponseOrDict', 'DeleteModelConfig', 'DeleteModelConfigDict', 'DeleteModelConfigOrDict', 'DeleteModelResponse', 'DeleteModelResponseDict', 'DeleteModelResponseOrDict', 'DeleteResourceJob', 'DeleteResourceJobDict', 'DeleteResourceJobOrDict', 'DistillationDataStats', 'DistillationDataStatsDict', 'DistillationDataStatsOrDict', 'DistillationHyperParameters', 'DistillationHyperParametersDict', 'DistillationHyperParametersOrDict', 'DistillationSpec', 'DistillationSpecDict', 'DistillationSpecOrDict', 'DownloadFileConfig', 'DownloadFileConfigDict', 'DownloadFileConfigOrDict', 'DynamicRetrievalConfig', 'DynamicRetrievalConfigDict', 'DynamicRetrievalConfigMode', 'DynamicRetrievalConfigOrDict', 'EditImageConfig', 'EditImageConfigDict', 'EditImageConfigOrDict', 'EditImageResponse', 'EditImageResponseDict', 'EditImageResponseOrDict', 'EditMode', 'EmbedContentConfig', 'EmbedContentConfigDict', 'EmbedContentConfigOrDict', 'EmbedContentMetadata', 'EmbedContentMetadataDict', 'EmbedContentMetadataOrDict', 'EmbedContentResponse', 'EmbedContentResponseDict', 'EmbedContentResponseOrDict', 'EncryptionSpec', 'EncryptionSpecDict', 'EncryptionSpecOrDict', 'EndSensitivity', 'Endpoint', 'EndpointDict', 'EndpointOrDict', 'Enum', 'EnumMeta', 'ExecutableCode', 'ExecutableCodeDict', 'ExecutableCodeOrDict', 'FeatureSelectionPreference', 'FetchPredictOperationConfig', 'FetchPredictOperationConfigDict', 'FetchPredictOperationConfigOrDict', 'Field', 'File', 'FileData', 'FileDataDict', 'FileDataOrDict', 'FileDict', 'FileOrDict', 'FileSource', 'FileState', 'FileStatus', 'FileStatusDict', 'FileStatusOrDict', 'FinishReason', 'FunctionCall', 'FunctionCallDict', 'FunctionCallOrDict', 'FunctionCallingConfig', 'FunctionCallingConfigDict', 'FunctionCallingConfigMode', 'FunctionCallingConfigOrDict', 'FunctionDeclaration', 'FunctionDeclarationDict', 'FunctionDeclarationOrDict', 'FunctionResponse', 'FunctionResponseDict', 'FunctionResponseOrDict', 'GenerateContentConfig', 'GenerateContentConfigDict', 'GenerateContentConfigOrDict', 'GenerateContentResponse', 'GenerateContentResponseDict', 'GenerateContentResponseOrDict', 'GenerateContentResponsePromptFeedback', 'GenerateContentResponsePromptFeedbackDict', 'GenerateContentResponsePromptFeedbackOrDict', 'GenerateContentResponseUsageMetadata', 'GenerateContentResponseUsageMetadataDict', 'GenerateContentResponseUsageMetadataOrDict', 'GenerateImagesConfig', 'GenerateImagesConfigDict', 'GenerateImagesConfigOrDict', 'GenerateImagesResponse', 'GenerateImagesResponseDict', 'GenerateImagesResponseOrDict', 'GenerateVideosConfig', 'GenerateVideosConfigDict', 'GenerateVideosConfigOrDict', 'GenerateVideosOperation', 'GenerateVideosOperationDict', 'GenerateVideosOperationOrDict', 'GenerateVideosResponse', 'GenerateVideosResponseDict', 'GenerateVideosResponseOrDict', 'GeneratedImage', 'GeneratedImageDict', 'GeneratedImageOrDict', 'GeneratedVideo', 'GeneratedVideoDict', 'GeneratedVideoOrDict', 'GenerationConfig', 'GenerationConfigDict', 'GenerationConfigOrDict', 'GenerationConfigRoutingConfig', 'GenerationConfigRoutingConfigAutoRoutingMode', 'GenerationConfigRoutingConfigAutoRoutingModeDict', 'GenerationConfigRoutingConfigAutoRoutingModeOrDict', 'GenerationConfigRoutingConfigDict', 'GenerationConfigRoutingConfigManualRoutingMode', 'GenerationConfigRoutingConfigManualRoutingModeDict', 'GenerationConfigRoutingConfigManualRoutingModeOrDict', 'GenerationConfigRoutingConfigOrDict', 'GetBatchJobConfig', 'GetBatchJobConfigDict', 'GetBatchJobConfigOrDict', 'GetCachedContentConfig', 'GetCachedContentConfigDict', 'GetCachedContentConfigOrDict', 'GetFileConfig', 'GetFileConfigDict', 'GetFileConfigOrDict', 'GetModelConfig', 'GetModelConfigDict', 'GetModelConfigOrDict', 'GetOperationConfig', 'GetOperationConfigDict', 'GetOperationConfigOrDict', 'GetTuningJobConfig', 'GetTuningJobConfigDict', 'GetTuningJobConfigOrDict', 'GoogleRpcStatus', 'GoogleRpcStatusDict', 'GoogleRpcStatusOrDict', 'GoogleSearch', 'GoogleSearchDict', 'GoogleSearchOrDict', 'GoogleSearchRetrieval', 'GoogleSearchRetrievalDict', 'GoogleSearchRetrievalOrDict', 'GoogleTypeDate', 'GoogleTypeDateDict', 'GoogleTypeDateOrDict', 'GroundingChunk', 'GroundingChunkDict', 'GroundingChunkOrDict', 'GroundingChunkRetrievedContext', 'GroundingChunkRetrievedContextDict', 'GroundingChunkRetrievedContextOrDict', 'GroundingChunkWeb', 'GroundingChunkWebDict', 'GroundingChunkWebOrDict', 'GroundingMetadata', 'GroundingMetadataDict', 'GroundingMetadataOrDict', 'GroundingSupport', 'GroundingSupportDict', 'GroundingSupportOrDict', 'HarmBlockMethod', 'HarmBlockThreshold', 'HarmCategory', 'HarmProbability', 'HarmSeverity', 'HttpOptions', 'HttpOptionsDict', 'HttpOptionsOrDict', 'Image', 'ImageDict', 'ImageOrDict', 'ImagePromptLanguage', 'JOB_STATES_ENDED', 'JOB_STATES_ENDED_MLDEV', 'JOB_STATES_ENDED_VERTEX', 'JOB_STATES_SUCCEEDED', 'JOB_STATES_SUCCEEDED_MLDEV', 'JOB_STATES_SUCCEEDED_VERTEX', 'JSONSchema', 'JSONSchemaType', 'JobError', 'JobErrorDict', 'JobErrorOrDict', 'JobState', 'Language', 'ListBatchJobsConfig', 'ListBatchJobsConfigDict', 'ListBatchJobsConfigOrDict', 'ListBatchJobsResponse', 'ListBatchJobsResponseDict', 'ListBatchJobsResponseOrDict', 'ListCachedContentsConfig', 'ListCachedContentsConfigDict', 'ListCachedContentsConfigOrDict', 'ListCachedContentsResponse', 'ListCachedContentsResponseDict', 'ListCachedContentsResponseOrDict', 'ListFilesConfig', 'ListFilesConfigDict', 'ListFilesConfigOrDict', 'ListFilesResponse', 'ListFilesResponseDict', 'ListFilesResponseOrDict', 'ListModelsConfig', 'ListModelsConfigDict', 'ListModelsConfigOrDict', 'ListModelsResponse', 'ListModelsResponseDict', 'ListModelsResponseOrDict', 'ListTuningJobsConfig', 'ListTuningJobsConfigDict', 'ListTuningJobsConfigOrDict', 'ListTuningJobsResponse', 'ListTuningJobsResponseDict', 'ListTuningJobsResponseOrDict', 'Literal', 'LiveClientContent', 'LiveClientContentDict', 'LiveClientContentOrDict', 'LiveClientMessage', 'LiveClientMessageDict', 'LiveClientMessageOrDict', 'LiveClientRealtimeInput', 'LiveClientRealtimeInputDict', 'LiveClientRealtimeInputOrDict', 'LiveClientSetup', 'LiveClientSetupDict', 'LiveClientSetupOrDict', 'LiveClientToolResponse', 'LiveClientToolResponseDict', 'LiveClientToolResponseOrDict', 'LiveConnectConfig', 'LiveConnectConfigDict', 'LiveConnectConfigOrDict', 'LiveConnectParameters', 'LiveConnectParametersDict', 'LiveConnectParametersOrDict', 'LiveSendRealtimeInputParameters', 'LiveSendRealtimeInputParametersDict', 'LiveSendRealtimeInputParametersOrDict', 'LiveServerContent', 'LiveServerContentDict', 'LiveServerContentOrDict', 'LiveServerGoAway', 'LiveServerGoAwayDict', 'LiveServerGoAwayOrDict', 'LiveServerMessage', 'LiveServerMessageDict', 'LiveServerMessageOrDict', 'LiveServerSessionResumptionUpdate', 'LiveServerSessionResumptionUpdateDict', 'LiveServerSessionResumptionUpdateOrDict', 'LiveServerSetupComplete', 'LiveServerSetupCompleteDict', 'LiveServerSetupCompleteOrDict', 'LiveServerToolCall', 'LiveServerToolCallCancellation', 'LiveServerToolCallCancellationDict', 'LiveServerToolCallCancellationOrDict', 'LiveServerToolCallDict', 'LiveServerToolCallOrDict', 'LogprobsResult', 'LogprobsResultCandidate', 'LogprobsResultCandidateDict', 'LogprobsResultCandidateOrDict', 'LogprobsResultDict', 'LogprobsResultOrDict', 'LogprobsResultTopCandidates', 'LogprobsResultTopCandidatesDict', 'LogprobsResultTopCandidatesOrDict', 'MaskReferenceConfig', 'MaskReferenceConfigDict', 'MaskReferenceConfigOrDict', 'MaskReferenceImage', 'MaskReferenceImageDict', 'MaskReferenceImageOrDict', 'MaskReferenceMode', 'MediaModality', 'MediaResolution', 'Modality', 'ModalityTokenCount', 'ModalityTokenCountDict', 'ModalityTokenCountOrDict', 'Mode', 'Model', 'ModelContent', 'ModelDict', 'ModelOrDict', 'ModelSelectionConfig', 'ModelSelectionConfigDict', 'ModelSelectionConfigOrDict', 'Operation', 'OperationDict', 'OperationOrDict', 'Optional', 'Outcome', 'PIL', 'PIL_Image', 'Part', 'PartDict', 'PartOrDict', 'PartUnion', 'PartUnionDict', 'PartnerModelTuningSpec', 'PartnerModelTuningSpecDict', 'PartnerModelTuningSpecOrDict', 'PersonGeneration', 'PrebuiltVoiceConfig', 'PrebuiltVoiceConfigDict', 'PrebuiltVoiceConfigOrDict', 'RagRetrievalConfig', 'RagRetrievalConfigDict', 'RagRetrievalConfigFilter', 'RagRetrievalConfigFilterDict', 'RagRetrievalConfigFilterOrDict', 'RagRetrievalConfigHybridSearch', 'RagRetrievalConfigHybridSearchDict', 'RagRetrievalConfigHybridSearchOrDict', 'RagRetrievalConfigOrDict', 'RagRetrievalConfigRanking', 'RagRetrievalConfigRankingDict', 'RagRetrievalConfigRankingLlmRanker', 'RagRetrievalConfigRankingLlmRankerDict', 'RagRetrievalConfigRankingLlmRankerOrDict', 'RagRetrievalConfigRankingOrDict', 'RagRetrievalConfigRankingRankService', 'RagRetrievalConfigRankingRankServiceDict', 'RagRetrievalConfigRankingRankServiceOrDict', 'RawReferenceImage', 'RawReferenceImageDict', 'RawReferenceImageOrDict', 'RealtimeInputConfig', 'RealtimeInputConfigDict', 'RealtimeInputConfigOrDict', 'ReplayFile', 'ReplayFileDict', 'ReplayFileOrDict', 'ReplayInteraction', 'ReplayInteractionDict', 'ReplayInteractionOrDict', 'ReplayRequest', 'ReplayRequestDict', 'ReplayRequestOrDict', 'ReplayResponse', 'ReplayResponseDict', 'ReplayResponseOrDict', 'Retrieval', 'RetrievalDict', 'RetrievalMetadata', 'RetrievalMetadataDict', 'RetrievalMetadataOrDict', 'RetrievalOrDict', 'SafetyAttributes', 'SafetyAttributesDict', 'SafetyAttributesOrDict', 'SafetyFilterLevel', 'SafetyRating', 'SafetyRatingDict', 'SafetyRatingOrDict', 'SafetySetting', 'SafetySettingDict', 'SafetySettingOrDict', 'Schema', 'SchemaDict', 'SchemaOrDict', 'SchemaUnion', 'SchemaUnionDict', 'SearchEntryPoint', 'SearchEntryPointDict', 'SearchEntryPointOrDict', 'Segment', 'SegmentDict', 'SegmentOrDict', 'Sequence', 'SessionResumptionConfig', 'SessionResumptionConfigDict', 'SessionResumptionConfigOrDict', 'SlidingWindow', 'SlidingWindowDict', 'SlidingWindowOrDict', 'SpeechConfig', 'SpeechConfigDict', 'SpeechConfigOrDict', 'SpeechConfigUnion', 'SpeechConfigUnionDict', 'StartSensitivity', 'StyleReferenceConfig', 'StyleReferenceConfigDict', 'StyleReferenceConfigOrDict', 'StyleReferenceImage', 'StyleReferenceImageDict', 'StyleReferenceImageOrDict', 'SubjectReferenceConfig', 'SubjectReferenceConfigDict', 'SubjectReferenceConfigOrDict', 'SubjectReferenceImage', 'SubjectReferenceImageDict', 'SubjectReferenceImageOrDict', 'SubjectReferenceType', 'SupervisedHyperParameters', 'SupervisedHyperParametersDict', 'SupervisedHyperParametersOrDict', 'SupervisedTuningDataStats', 'SupervisedTuningDataStatsDict', 'SupervisedTuningDataStatsOrDict', 'SupervisedTuningDatasetDistribution', 'SupervisedTuningDatasetDistributionDatasetBucket', 'SupervisedTuningDatasetDistributionDatasetBucketDict', 'SupervisedTuningDatasetDistributionDatasetBucketOrDict', 'SupervisedTuningDatasetDistributionDict', 'SupervisedTuningDatasetDistributionOrDict', 'SupervisedTuningSpec', 'SupervisedTuningSpecDict', 'SupervisedTuningSpecOrDict', 'T', 'TestTableFile', 'TestTableFileDict', 'TestTableFileOrDict', 'TestTableItem', 'TestTableItemDict', 'TestTableItemOrDict', 'ThinkingConfig', 'ThinkingConfigDict', 'ThinkingConfigOrDict', 'TokensInfo', 'TokensInfoDict', 'TokensInfoOrDict', 'Tool', 'ToolCodeExecution', 'ToolCodeExecutionDict', 'ToolCodeExecutionOrDict', 'ToolConfig', 'ToolConfigDict', 'ToolConfigOrDict', 'ToolDict', 'ToolListUnion', 'ToolListUnionDict', 'ToolOrDict', 'TrafficType', 'Transcription', 'TranscriptionDict', 'TranscriptionOrDict', 'TunedModel', 'TunedModelDict', 'TunedModelInfo', 'TunedModelInfoDict', 'TunedModelInfoOrDict', 'TunedModelOrDict', 'TuningDataStats', 'TuningDataStatsDict', 'TuningDataStatsOrDict', 'TuningDataset', 'TuningDatasetDict', 'TuningDatasetOrDict', 'TuningExample', 'TuningExampleDict', 'TuningExampleOrDict', 'TuningJob', 'TuningJobDict', 'TuningJobOrDict', 'TuningValidationDataset', 'TuningValidationDatasetDict', 'TuningValidationDatasetOrDict', 'TurnCoverage', 'Type', 'TypedDict', 'Union', 'UpdateCachedContentConfig', 'UpdateCachedContentConfigDict', 'UpdateCachedContentConfigOrDict', 'UpdateModelConfig', 'UpdateModelConfigDict', 'UpdateModelConfigOrDict', 'UploadFileConfig', 'UploadFileConfigDict', 'UploadFileConfigOrDict', 'UpscaleImageConfig', 'UpscaleImageConfigDict', 'UpscaleImageConfigOrDict', 'UpscaleImageParameters', 'UpscaleImageParametersDict', 'UpscaleImageParametersOrDict', 'UpscaleImageResponse', 'UpscaleImageResponseDict', 'UpscaleImageResponseOrDict', 'UsageMetadata', 'UsageMetadataDict', 'UsageMetadataOrDict', 'UserContent', 'VersionedUnionType', 'VertexAISearch', 'VertexAISearchDict', 'VertexAISearchOrDict', 'VertexRagStore', 'VertexRagStoreDict', 'VertexRagStoreOrDict', 'VertexRagStoreRagResource', 'VertexRagStoreRagResourceDict', 'VertexRagStoreRagResourceOrDict', 'Video', 'VideoDict', 'VideoMetadata', 'VideoMetadataDict', 'VideoMetadataOrDict', 'VideoOrDict', 'VoiceConfig', 'VoiceConfigDict', 'VoiceConfigOrDict', '_CancelBatchJobParameters', '_CancelBatchJobParametersDict', '_CancelBatchJobParametersOrDict', '_ComputeTokensParameters', '_ComputeTokensParametersDict', '_ComputeTokensParametersOrDict', '_CountTokensParameters', '_CountTokensParametersDict', '_CountTokensParametersOrDict', '_CreateBatchJobParameters', '_CreateBatchJobParametersDict', '_CreateBatchJobParametersOrDict', '_CreateCachedContentParameters', '_CreateCachedContentParametersDict', '_CreateCachedContentParametersOrDict', '_CreateFileParameters', '_CreateFileParametersDict', '_CreateFileParametersOrDict', '_CreateTuningJobParameters', '_CreateTuningJobParametersDict', '_CreateTuningJobParametersOrDict', '_DeleteBatchJobParameters', '_DeleteBatchJobParametersDict', '_DeleteBatchJobParametersOrDict', '_DeleteCachedContentParameters', '_DeleteCachedContentParametersDict', '_DeleteCachedContentParametersOrDict', '_DeleteFileParameters', '_DeleteFileParametersDict', '_DeleteFileParametersOrDict', '_DeleteModelParameters', '_DeleteModelParametersDict', '_DeleteModelParametersOrDict', '_EditImageParameters', '_EditImageParametersDict', '_EditImageParametersOrDict', '_EmbedContentParameters', '_EmbedContentParametersDict', '_EmbedContentParametersOrDict', '_FetchPredictOperationParameters', '_FetchPredictOperationParametersDict', '_FetchPredictOperationParametersOrDict', '_GenerateContentParameters', '_GenerateContentParametersDict', '_GenerateContentParametersOrDict', '_GenerateImagesParameters', '_GenerateImagesParametersDict', '_GenerateImagesParametersOrDict', '_GenerateVideosParameters', '_GenerateVideosParametersDict', '_GenerateVideosParametersOrDict', '_GetBatchJobParameters', '_GetBatchJobParametersDict', '_GetBatchJobParametersOrDict', '_GetCachedContentParameters', '_GetCachedContentParametersDict', '_GetCachedContentParametersOrDict', '_GetFileParameters', '_GetFileParametersDict', '_GetFileParametersOrDict', '_GetModelParameters', '_GetModelParametersDict', '_GetModelParametersOrDict', '_GetOperationParameters', '_GetOperationParametersDict', '_GetOperationParametersOrDict', '_GetTuningJobParameters', '_GetTuningJobParametersDict', '_GetTuningJobParametersOrDict', '_ListBatchJobsParameters', '_ListBatchJobsParametersDict', '_ListBatchJobsParametersOrDict', '_ListCachedContentsParameters', '_ListCachedContentsParametersDict', '_ListCachedContentsParametersOrDict', '_ListFilesParameters', '_ListFilesParametersDict', '_ListFilesParametersOrDict', '_ListModelsParameters', '_ListModelsParametersDict', '_ListModelsParametersOrDict', '_ListTuningJobsParameters', '_ListTuningJobsParametersDict', '_ListTuningJobsParametersOrDict', '_ReferenceImageAPI', '_ReferenceImageAPIDict', '_ReferenceImageAPIOrDict', '_UNION_TYPES', '_UnionGenericAlias', '_UpdateCachedContentParameters', '_UpdateCachedContentParametersDict', '_UpdateCachedContentParametersOrDict', '_UpdateModelParameters', '_UpdateModelParametersDict', '_UpdateModelParametersOrDict', '_UpscaleImageAPIConfig', '_UpscaleImageAPIConfigDict', '_UpscaleImageAPIConfigOrDict', '_UpscaleImageAPIParameters', '_UpscaleImageAPIParametersDict', '_UpscaleImageAPIParametersOrDict', '__annotations__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '_common', '_is_pillow_image_imported', 'builtin_types', 'datetime', 'inspect', 'json', 'logger', 'logging', 'pydantic', 'sys', 'typing']\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "from google.genai import Client, types\n",
    "import time\n",
    "\n",
    "print(dir(genai))\n",
    "print(dir(Client))\n",
    "print(dir(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0389390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded image metadata successfully.\n",
      "Sending request for image_id: 710UiL6+naL\n",
      "Processed image_id: 710UiL6+naL\n",
      "Sleeping 60 seconds to respect rate limits...\n"
     ]
    }
   ],
   "source": [
    "client = genai.Client(api_key=\"AIzaSyDjhwayg-2ZeODeh0O6VNN-gZ57U4EbfmQ\") #goyalkeshav\n",
    "\n",
    "# Set daily limits\n",
    "MAX_DAILY_REQUESTS = 1500\n",
    "DELAY_BETWEEN_REQUESTS = 60 \n",
    "\n",
    "requests_made = 0\n",
    "\n",
    "def load_progress(progress_file):\n",
    "    if os.path.exists(progress_file):\n",
    "        with open(progress_file, 'r') as f:\n",
    "            index = f.read().strip()\n",
    "            return int(index)\n",
    "    return 0\n",
    "\n",
    "def save_progress(progress_file, index):\n",
    "    with open(progress_file, 'w') as f:\n",
    "        f.write(str(index))\n",
    "\n",
    "def query_gemini_api(image_bytes, combined_description):\n",
    "    prompt_text = (\n",
    "        \"You are given an image and a brief product description.\\n\"\n",
    "        f\"Use the product description context: {combined_description}\\n\"\n",
    "        \"Generate exactly 5 diverse, visually clear, and progressively challenging questions.\\n\"\n",
    "        \"Each question must be answerable by only looking at the image — do NOT rely on external or assumed knowledge.\\n\"\n",
    "        \"Ensure variation in the *type* of visual cues used: color, shape, count, spatial relationship, relative size, and visible text (if any).\\n\"\n",
    "        \"Ensure variation in *difficulty level*:\\n\"\n",
    "        \"- At least 2 simple questions (e.g., color, count)\\n\"\n",
    "        \"- At least 2 moderately difficult questions (e.g., spatial relations, comparisons)\\n\"\n",
    "        \"- 1 challenging question requiring closer inspection or subtle visual reasoning (e.g., most prominent item, inferred use from shape)\\n\"\n",
    "        \"Do NOT ask about materials or properties that are not visually obvious (e.g., plastic, flexible, metal).\\n\"\n",
    "        \"Answers must be a single word — not all of them 'yes' or 'no'.\\n\"\n",
    "        \"Strictly use this format without extra text:\\n\"\n",
    "        \"Question 1: <question>\\n\"\n",
    "        \"Answer 1: <answer>\\n\"\n",
    "        \"Do not include any explanations or extra text.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.0-flash',\n",
    "            contents=[\n",
    "                types.Part.from_bytes(\n",
    "                    data=image_bytes,\n",
    "                    mime_type='image/jpeg'\n",
    "                ),\n",
    "                prompt_text\n",
    "            ]\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Gemini API: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file):\n",
    "    global requests_made\n",
    "\n",
    "    image_path_map = {}\n",
    "    with open(images_csv_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            image_path_map[row['image_id']] = row['path']\n",
    "\n",
    "    print(\"Loaded image metadata successfully.\")\n",
    "\n",
    "    output_dir = os.path.dirname(output_file)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Load last processed index\n",
    "    start_index = load_progress(progress_file)\n",
    "    current_index = 0\n",
    "\n",
    "    with open(output_file, 'a', newline='', encoding='utf-8') as f_out:\n",
    "        writer = csv.writer(f_out)\n",
    "        if os.stat(output_file).st_size == 0:\n",
    "            writer.writerow(['image_id', 'full_image_path', 'question', 'answer'])\n",
    "\n",
    "        with open(listings_csv_path, 'r', encoding='utf-8') as f_in:\n",
    "            reader = csv.DictReader(f_in)\n",
    "            for row in reader:\n",
    "                if current_index < start_index:\n",
    "                    current_index += 1\n",
    "                    continue  # skip already processed\n",
    "\n",
    "                if requests_made >= MAX_DAILY_REQUESTS:\n",
    "                    print(\"Reached daily request limit. Stopping.\")\n",
    "                    break\n",
    "\n",
    "                image_id = row['main_image_id']\n",
    "                image_filename = image_path_map.get(image_id)\n",
    "\n",
    "                if not image_filename:\n",
    "                    print(f\"Image path not found for image_id: {image_id}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                full_image_path = os.path.join(images_base_path, image_filename)\n",
    "\n",
    "                if not os.path.exists(full_image_path):\n",
    "                    print(f\"Image file does not exist: {full_image_path}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    with open(full_image_path, \"rb\") as img_file:\n",
    "                        image_bytes = img_file.read()\n",
    "                except Exception as e:\n",
    "                    print(f\"Failed to read image {full_image_path}: {e}\")\n",
    "                    current_index += 1\n",
    "                    continue\n",
    "\n",
    "                combined_description = f\"Overall: {row['overall_description']}; \" \\\n",
    "                                       f\"Color: {row['colour_description']}; \" \\\n",
    "                                       f\"Material: {row['material_description']}\"\\\n",
    "                                       f\"Other: {row['other_description']}; \" \\\n",
    "\n",
    "                print(f\"Sending request for image_id: {image_id}\")\n",
    "\n",
    "                generated_text = query_gemini_api(image_bytes, combined_description)\n",
    "\n",
    "                if generated_text:\n",
    "                    lines = [line.strip() for line in generated_text.strip().split('\\n') if line.strip()]\n",
    "                    question_lines = [line for line in lines if line.lower().startswith('question')]\n",
    "                    answer_lines = [line for line in lines if line.lower().startswith('answer')]\n",
    "\n",
    "                    if len(question_lines) == 5 and len(answer_lines) == 5:\n",
    "                        for q_line, a_line in zip(question_lines, answer_lines):\n",
    "                            question = q_line.split(':', 1)[1].strip()\n",
    "                            answer = a_line.split(':', 1)[1].strip()\n",
    "                            writer.writerow([image_id, full_image_path, question, answer])\n",
    "                            f_out.flush()\n",
    "                        print(f\"Processed image_id: {image_id}\")\n",
    "                    else:\n",
    "                        print(f\"Unexpected format or count in response for image_id: {image_id}\")\n",
    "                else:\n",
    "                    print(f\"Failed to generate questions for image_id: {image_id}\")\n",
    "\n",
    "                requests_made += 1\n",
    "                current_index += 1\n",
    "                save_progress(progress_file, current_index)\n",
    "\n",
    "                if requests_made < MAX_DAILY_REQUESTS:\n",
    "                    print(f\"Sleeping {DELAY_BETWEEN_REQUESTS} seconds to respect rate limits...\")\n",
    "                    time.sleep(DELAY_BETWEEN_REQUESTS)\n",
    "\n",
    "import os\n",
    "\n",
    "# CONFIGURATION\n",
    "################\n",
    "# ye change karna hai\n",
    "current_working_filename = 'listings_3'\n",
    "question_set_number = 'set_4'\n",
    "\n",
    "\n",
    "listings_csv_path = f'abo-listings/listings/filtered_metadata/{current_working_filename}.csv'\n",
    "images_csv_path = 'abo-images-small/images/metadata/images.csv'\n",
    "images_base_path = 'abo-images-small/images/small'\n",
    "\n",
    "output_dir = 'generated_questions'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "output_file = os.path.join(output_dir, f'questions_{current_working_filename}_{question_set_number}.csv')\n",
    "\n",
    "progress_dir = 'progress'\n",
    "os.makedirs(progress_dir, exist_ok=True)\n",
    "progress_file = os.path.join(progress_dir, f'progress_{current_working_filename}.txt')\n",
    "\n",
    "\n",
    "process_records(listings_csv_path, images_csv_path, images_base_path, output_file, progress_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515559e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# for example i have to do for listings 0,1 and 2, then i guess 3 parallel ipynb notebooks par sirf ye wala cell\n",
    "# run karlenge, then:\n",
    "\n",
    "### day 1:\n",
    "## for first notebook or py file : current_working_filename = 'listings_0' and question_set_number = 'set_1'\n",
    "## for second notebook or py file : current_working_filename = 'listings_1' and question_set_number = 'set_1'\n",
    "## for third notebook or py file : current_working_filename = 'listings_2' and question_set_number = 'set_1'\n",
    "\n",
    "### day 2:\n",
    "## for first notebook or py file : current_working_filename = 'listings_0' and question_set_number = 'set_2'\n",
    "## for second notebook or py file : current_working_filename = 'listings_1' and question_set_number = 'set_2'\n",
    "## for third notebook or py file : current_working_filename = 'listings_2' and question_set_number = 'set_2'\n",
    "\n",
    "### day 3:\n",
    "## for first notebook or py file : current_working_filename = 'listings_0' and question_set_number = 'set_3'\n",
    "## for second notebook or py file : current_working_filename = 'listings_1' and question_set_number = 'set_3'\n",
    "## for third notebook or py file : current_working_filename = 'listings_2' and question_set_number = 'set_3'\n",
    "\n",
    "######\n",
    "# for a particular lisings file for eg listings_x.csv, \"progress/progress_listings_x.txt\" mein progress save hojayega\n",
    "# so that next din bhi code sahi record se run ho and repetetively process naa kare records ko\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#############################################################\n",
    "# Himanshu: listings 0,1,2\n",
    "# Keshav: listings 3,4,5\n",
    "# Uttam: listings 6,7,8\n",
    "# Pranav: listings a,b,c\n",
    "############################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLProject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
